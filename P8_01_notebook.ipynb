{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, split, udf\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "current_credentials = credentials.get_frozen_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('P8').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id=current_credentials.access_key,\n",
    "                  aws_secret_access_key=current_credentials.secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get images paths\n",
    "\n",
    "img_path = []\n",
    "for key in s3.list_objects(Bucket='fruits-oc-projet-8')['Contents']:\n",
    "    img_path.append(key['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = map(lambda x : Row(x), img_path)\n",
    "df = spark.createDataFrame(img_path, ['img_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get category\n",
    "\n",
    "df = df.withColumn('category', split(df['img_path'], '/').getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False, weights='imagenet', pooling='avg')\n",
    "model.summary() # verify that the top layer is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = ResNet50(weights='imagenet', pooling='avg', include_top=False)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_path):\n",
    "    \"\"\"\n",
    "    Preprocesses image for prediction.\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3',\n",
    "                        aws_access_key_id=current_credentials.access_key,\n",
    "                        aws_secret_access_key= current_credentials.secret_key)\n",
    "    \n",
    "    content = s3.Object(bucket_name='fruits-oc-projet-8', key=img_path)\n",
    "    cont = content.get()\n",
    "    body = cont['Body']\n",
    "    \n",
    "    img = Image.open(body).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas UDFs on large records (e.g., very large images) can run into Out Of Memory (OOM) errors.\n",
    "# If you hit such errors in the cell below, try reducing the Arrow batch size via `maxRecordsPerBatch`.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now run featurization on our entire Spark DataFrame.\n",
    "# NOTE: This can take a long time (about 10 minutes) since it applies a large model to the full dataset.\n",
    "features_df = df.repartition(16).select(col('img_path'), col('category'), featurize_udf('img_path').alias('features'))\n",
    "features_df = features_df.withColumn(\"features_udf\", list_to_vector_udf(col(\"features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features scaling before PCA application\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_udf\", outputCol=\"features_udf_sca\")\n",
    "features_df = scaler.fit((features_df)).transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=50, inputCol=\"features_udf_sca\", outputCol=\"pcaFeatures_udf\")\n",
    "model = pca.fit(features_df)\n",
    "result = model.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.select(col('img_path'), col('category'), col('pcaFeatures_udf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Uploading results on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket='ocprojet8result', CreateBucketConfiguration={'LocationConstraint': 'eu-west-3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "result_pd.to_csv(csv_buffer)\n",
    "\n",
    "response = s3.put_object(\n",
    "    ACL='public-read',\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    Bucket='ocprojet8result',\n",
    "    Key='result.csv'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
